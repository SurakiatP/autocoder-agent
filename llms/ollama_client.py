# llms/ollama_client.py

import requests

# Ollama API base URL
OLLAMA_API_URL = "http://localhost:11434/api/generate"

# Default model to use (you can change this to e.g. "deepseek-coder")
MODEL_NAME = "wizardcoder"

def generate_with_ollama(prompt: str, max_tokens: int = 512) -> str:
    """
    Generate a response from an LLM running on Ollama (e.g., WizardCoder).

    Args:
        prompt (str): The prompt to send to the LLM.
        max_tokens (int): Maximum number of tokens to generate.

    Returns:
        str: The response generated by the LLM.
    """
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": max_tokens
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload)
        response.raise_for_status()
        return response.json()["response"].strip()
    except Exception as e:
        return f"[Error contacting Ollama]: {e}"
